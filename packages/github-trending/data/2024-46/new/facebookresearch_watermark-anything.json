{
  "name": "facebookresearch/watermark-anything",
  "description": "Official implementation of the paper \"Watermark Anything with Localized Messages\"",
  "stars": 724,
  "forks": 16,
  "watchers": 724,
  "mainLanguage": "Jupyter Notebook",
  "languages": {
    "Jupyter Notebook": 9351481,
    "Python": 253347
  },
  "topics": [
    "image",
    "watermarking"
  ],
  "readme": "# üê§ Watermark Anything\n\nImplementation and pretrained models for the paper [**Watermark Anything**](https://arxiv.org/abs/2411.07231). \nOur approach allows for embedding (possibly multiple) localized watermarks into images.\n\n<!-- [[`Webpage`](...)] -->\n[[`arXiv`](https://arxiv.org/abs/2411.07231)]\n[[`Colab`](https://colab.research.google.com/github/facebookresearch/watermark-anything/blob/main/notebooks/colab.ipynb)]\n[[`Podcast`](https://notebooklm.google.com/notebook/6c69b3f8-b1a6-41c4-92fb-c416903ceb49/audio)]\n[[`HN`](https://news.ycombinator.com/item?id=42113674)]\n\n![Watermark Anything Overview](assets/splash_wam.jpg)\n\n\n## Requirements\n\n\n### Installation\n\nThis repos was tested with Python 3.10.14, PyTorch 2.5.1, CUDA 12.4, Torchvision 0.20.1:\n```cmd\nconda create -n \"watermark_anything\" python=3.10.14\nconda activate watermark_anything\nconda install pytorch torchvision pytorch-cuda=12.4 -c pytorch -c nvidia\n```\n\nInstall the required packages:\n```cmd\npip install -r requirements.txt\n```\n\n### Weights\n\nDownload the pre-trained model weights [here](https://dl.fbaipublicfiles.com/watermark_anything/checkpoint.pth), or via command line:\n```cmd\nwget https://dl.fbaipublicfiles.com/watermark_anything/checkpoint.pth -P checkpoints/ -P checkpoints/\n```\n\n### Data\n\nFor training our models we use the [COCO](https://cocodataset.org/#home) dataset, with additional safety filters and where faces are blurred.\n\n## Inference\n\nSee `notebooks/inference.ipynb` for a notebook with the following scripts as well as vizualizations.\n\n<details>\n<summary>Imports, load model and specify folder with images to watermark:</summary>\n<br>\n\n```py\nimport os\nimport numpy as np\nfrom PIL import Image\nimport torch\nimport torch.nn.functional as F\nfrom torchvision.utils import save_image\n\nfrom watermark_anything.data.metrics import msg_predict_inference\nfrom notebooks.inference_utils import (\n    load_model_from_checkpoint, default_transform, unnormalize_img,\n    create_random_mask, plot_outputs, msg2str\n)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the model from the specified checkpoint\nexp_dir = \"checkpoints\"\njson_path = os.path.join(exp_dir, \"params.json\")\nckpt_path = os.path.join(exp_dir, 'checkpoint.pth')\nwam = load_model_from_checkpoint(json_path, ckpt_path).to(device).eval()\n\n# Define the directory containing the images to watermark\nimg_dir = \"assets/images\"  # Directory containing the original images\noutput_dir = \"outputs\"  # Directory to save the watermarked images\nos.makedirs(output_dir, exist_ok=True)\n```\n</details>\n\n> [!TIP]\n> You can specify the `wam.scaling_w` factor, which controls the imperceptibility/robustness trade-off. Increasing it will lead to worse images but more robust watermarks, and vice versa.\n> By default, it is set to 2.0, feel free to increase or decrease it to test how it influences the metrics.\n\n\n### Single Watermark\n\nExample of script for watermark embedding, detection and decoding for one message:\n\n```py \n# Define a 32-bit message to be embedded into the images\nwm_msg = torch.randint(0, 2, (32,)).float().to(device)\n\n# Proportion of the image to be watermarked (0.5 means 50% of the image).\n# This is used here to show the watermark localization property. In practice, you may want to use a predifined mask or the entire image.\nproportion_masked = 0.5\n\n# Iterate over each image in the directory\nfor img_ in os.listdir(img_dir):\n    # Load and preprocess the image\n    img_path = os.path.join(img_dir, img_)\n    img = Image.open(img_path).convert(\"RGB\")\n    img_pt = default_transform(img).unsqueeze(0).to(device)  # [1, 3, H, W]\n    \n    # Embed the watermark message into the image\n    outputs = wam.embed(img_pt, wm_msg)\n\n    # Create a random mask to watermark only a part of the image\n    mask = create_random_mask(img_pt, num_masks=1,mask_percentage=proportion_masked)  # [1, 1, H, W]\n    img_w = outputs['imgs_w'] * mask + img_pt * (1 - mask)  # [1, 3, H, W]\n\n    # Detect the watermark in the watermarked image\n    preds = wam.detect(img_w)[\"preds\"]  # [1, 33, 256, 256]\n    mask_preds = F.sigmoid(preds[:, 0, :, :])  # [1, 256, 256], predicted mask\n    bit_preds = preds[:, 1:, :, :]  # [1, 32, 256, 256], predicted bits\n    \n    # Predict the embedded message and calculate bit accuracy\n    pred_message = msg_predict_inference(bit_preds, mask_preds).cpu().float()  # [1, 32]\n    bit_acc = (pred_message == wm_msg).float().mean().item()\n\n    # Save the watermarked image and the detection mask\n    mask_preds_res = F.interpolate(mask_preds.unsqueeze(1), size=(img_pt.shape[-2], img_pt.shape[-1]), mode=\"bilinear\", align_corners=False)  # [1, 1, H, W]\n    save_image(unnormalize_img(img_w), f\"{output_dir}/{img_}_wm.png\")\n    save_image(mask_preds_res, f\"{output_dir}/{img_}_pred.png\")\n    save_image(mask, f\"{output_dir}/{img_}_target.png\")\n    \n    # Print the predicted message and bit accuracy for each image\n    print(f\"Predicted message for image {img_}: \", pred_message[0].numpy())\n    print(f\"Bit accuracy for image {img_}: \", bit_acc)\n```\n\n\n### Multiple Watermarks\n\n\n<details>\n<summary>Example of script for watermark embedding, detection and decoding for multiple messages:</summary>\n<br>\n\n```py \nfrom inference_utils import multiwm_dbscan\n\n# DBSCAN parameters for detection\nepsilon = 1 # min distance between decoded messages in a cluster\nmin_samples = 500 # min number of pixels in a 256x256 image to form a cluster\n\n# multiple 32 bit message to hide (could be more than 2; does not have to be 1 minus the other)\nwm_msgs = torch.randint(0, 2, (2, 32)).float().to(device)\nproportion_masked = 0.1 # max proportion per watermark, randomly placed\n\nfor img_ in os.listdir(img_dir):\n    img = os.path.join(img_dir, img_)\n    img = Image.open(img, \"r\").convert(\"RGB\")  \n    img_pt = default_transform(img).unsqueeze(0).to(device)\n    # Mask to use. 1 values correspond to pixels where the watermark will be placed.\n    masks = create_random_mask(img_pt, num_masks=len(wm_msgs), mask_percentage=proportion_masked)  # create one random mask per message\n    multi_wm_img = img_pt.clone()\n    for ii in range(len(wm_msgs)):\n        wm_msg, mask = wm_msgs[ii].unsqueeze(0), masks[ii]\n        outputs = wam.embed(img_pt, wm_msg) \n        multi_wm_img = outputs['imgs_w'] * mask + multi_wm_img * (1 - mask)  # [1, 3, H, W]\n\n    # Detect the watermark in the multi-watermarked image\n    preds = wam.detect(multi_wm_img)[\"preds\"]  # [1, 33, 256, 256]\n    mask_preds = F.sigmoid(preds[:, 0, :, :])  # [1, 256, 256], predicted mask\n    bit_preds = preds[:, 1:, :, :]  # [1, 32, 256, 256], predicted bits\n\n    # positions has the cluster number at each pixel. can be upsaled back to the original size.\n    centroids, positions = multiwm_dbscan(bit_preds, mask_preds, epsilon = epsilon, min_samples = min_samples)\n    centroids_pt = torch.stack(list(centroids.values()))\n\n    print(f\"number messages found in image {img_}: {len(centroids)}\")\n    for centroid in centroids_pt:\n        print(f\"found centroid: {msg2str(centroid)}\")\n        bit_acc = (centroid == wm_msgs).float().mean(dim=1)\n        # get message with maximum bit accuracy\n        bit_acc, idx = bit_acc.max(dim=0)\n        hamming = int(torch.sum(centroid != wm_msgs[idx]).item())\n        print(f\"bit accuracy: {bit_acc.item()} - hamming distance: {hamming}/{len(wm_msgs[0])}\")\n```\n</details>\n\n## Training\n\n### Pretraining\n\nPretraining for robustness:\n\n```cmd\ntorchrun --nproc_per_node=2  train.py \\\n    --local_rank -1  --output_dir <PRETRAINING_OUTPUT_DIRECTORY> \\\n    --augmentation_config configs/all_augs.yaml --extractor_model sam_base --embedder_model vae_small \\\n    --img_size 256 --batch_size 16 --batch_size_eval 32 --epochs 300 \\\n    --optimizer \"AdamW,lr=5e-5\" --scheduler \"CosineLRScheduler,lr_min=1e-6,t_initial=300,warmup_lr_init=1e-6,warmup_t=10\" \\\n    --seed 42 --perceptual_loss none --lambda_i 0.0 --lambda_d 0.0 --lambda_w 1.0 --lambda_w2 10.0 \\\n    --nbits 32 --scaling_i 1.0 --scaling_w 0.3 \\\n    --train_dir <COCO_TRAIN_DIRECTORY_PATH> --train_annotation_file <COCO_TRAIN_ANNOTATION_FILE_PATH> \\\n    --val_dir <COCO_VALIDATION_DIRECTORY_PATH> --val_annotation_file <COCO_VALIDATION_ANNOTATION_FILE_PATH> \n```\n\n\n### Finetuning for Multiple Watermarks and Imperceptibility\n\nFinetuning the model for handling multiple watermarks and ensuring imperceptibility:\n```cmd\ntorchrun --nproc_per_node=8 train.py \\\n    --local_rank 0 --debug_slurm --output_dir <FINETUNING_OUTPUT_DIRECTORY>\\\n    --augmentation_config configs/all_augs_multi_wm.yaml --extractor_model sam_base --embedder_model vae_small \\\n    --resume_from <PRETRAINING_OUTPUT_DIRECTORY>/checkpoint.pth \\\n    --attenuation jnd_1_3_blue --img_size 256 --batch_size 8 --batch_size_eval 16 --epochs 200 \\\n    --optimizer \"AdamW,lr=1e-4\" --scheduler \"CosineLRScheduler,lr_min=1e-6,t_initial=100,warmup_lr_init=1e-6,warmup_t=5\" \\\n    --seed 42 --perceptual_loss none --lambda_i 0 --lambda_d 0 --lambda_w 1.0 --lambda_w2 6.0 \\\n    --nbits 32 --scaling_i 1.0 --scaling_w 2.0 --multiple_w 1 --roll_probability 0.2 \\\n    --train_dir <COCO_TRAIN_DIRECTORY_PATH> --train_annotation_file <COCO_TRAIN_ANNOTATION_FILE_PATH> \\\n    --val_dir <COCO_VALIDATION_DIRECTORY_PATH> --val_annotation_file <COCO_VALIDATION_ANNOTATION_FILE_PATH>\n```\n\n\n## License\n\nThe model is licensed under the [CC-BY-NC](LICENSE).\n\n## Contributing\n\nSee [contributing](.github/CONTRIBUTING.md) and the [code of conduct](.github/CODE_OF_CONDUCT.md).\n\n## See Also\n\n- [**AudioSeal**](https://github.com/facebookresearch/audioseal)\n- [**Segment Anything**](https://github.com/facebookresearch/segment-anything/)\n\n## Citation\n\nIf you find this repository useful, please consider giving a star :star: and please cite as:\n\n```bibtex\n@article{sander2024watermark,\n  title={Watermark Anything with Localized Messages},\n  author={Sander, Tom and Fernandez, Pierre and Durmus, Alain and Furon, Teddy and Douze, Matthijs},\n  journal={arXiv preprint arXiv:2411.07231},\n  year={2024}\n}\n",
  "homepage": "",
  "defaultBranch": "main",
  "hasIssues": true,
  "hasWiki": false,
  "license": "Other",
  "createdAt": "2024-11-11T23:28:26Z",
  "updatedAt": "2024-11-17T09:32:34Z",
  "pushedAt": "2024-11-15T17:05:36Z",
  "url": "https://github.com/facebookresearch/watermark-anything",
  "gitUrl": "git://github.com/facebookresearch/watermark-anything.git",
  "sshUrl": "git@github.com:facebookresearch/watermark-anything.git",
  "owner": {
    "name": "facebookresearch",
    "avatarUrl": "https://avatars.githubusercontent.com/u/16943930?v=4",
    "url": "https://github.com/facebookresearch"
  }
}